{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c776a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "import sys\n",
    "from gensim.models import word2vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the given dataset using pandas\n",
    "text_data = pd.read_csv(\"Precily_Text_Similarity.csv\")\n",
    "print(\"Shape of text_data: \", text_data.shape)\n",
    "text_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872cda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if text data has any null values\n",
    "text_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd6333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text1\n",
    "preprocessed_text1 = []\n",
    "for sentence in tqdm(text_data['text1'].values):\n",
    "    sent = decontracted(sentence)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "    preprocessed_text1.append(sent.lower().strip())\n",
    "text_data['text1'] = preprocessed_text1\n",
    "text_data.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text2\n",
    "preprocessed_text2 = []\n",
    "for sentence in tqdm(text_data['text2'].values):\n",
    "    sent = decontracted(sentence)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "    preprocessed_text2.append(sent.lower().strip())\n",
    "text_data['text2'] = preprocessed_text2\n",
    "text_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d341e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Google News Vectors after downloading the file\n",
    "wordmodelfile = \"GoogleNews-vectors-negative300.bin\"\n",
    "wordmodel = gensim.models.KeyedVectors.load_word2vec_format(wordmodelfile, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(wordmodel.word2vec.key_to_index.keys())\n",
    "self.word2vec = {word:wordmodel.word2vec[word]%EMBEDDING_DIM for word in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccedd73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate similarity scores\n",
    "similarity = []\n",
    "for ind in text_data.index:\n",
    "    s1 = text_data['text1'][ind]\n",
    "    s2 = text_data['text2'][ind]\n",
    "\n",
    "    if s1 == s2:\n",
    "        similarity.append(0.0)  # 0 means highly similar\n",
    "    else:\n",
    "        s1words = word_tokenizer(s1)\n",
    "        s2words = word_tokenizer(s2)\n",
    "        vocab = wordmodel.vocab  # the vocabulary considered in the word embeddings\n",
    "\n",
    "        if len(s1words and s2words) == 0:\n",
    "            similarity.append(1.0)\n",
    "        else:\n",
    "            for word in s1words.copy():\n",
    "                if word not in vocab:\n",
    "                    s1words.remove(word)\n",
    "\n",
    "            for word in s2words.copy():\n",
    "                if word not in vocab:\n",
    "                    s2words.remove(word)\n",
    "\n",
    "            similarity.append(\n",
    "                (1 - wordmodel.n_similarity(s1words, s2words)))  # 1 means highly dissimilar, 0 means highly similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377906c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "  >>> import nltk\n",
    "  >>> nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0551d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with Unique_ID and similarity scores\n",
    "final_score = pd.DataFrame({'Unique_ID': text_data.Unique_ID, 'Similarity_score': similarity})\n",
    "final_score.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eec5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a CSV file\n",
    "final_score.to_csv('final_score.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
